**General Continuous Projection Memory (GCPM) — Строгая математическая формализация**

---

## Общая идея

GCPM рассматривает память как набор направленных вкладов в пространстве $\ell = (\vec{o}, \vec{d}) \in \mathbb{R}^{2D}$, где каждая запись представлена как направленный объёмный элемент с определённым положением, направлением, формой (через аксиальное и радиальное затухание) и смысловым содержимым. Память реализует волновое затухающее воздействие, локализованное в пространстве проекций и пригодное для точного, аналитически управляемого чтения, обновления, забывания и поиска.

---

## Структура памяти

$$
\mathcal{M} = \left\{ \left(\vec{o}_j, \vec{d}_j, T_j, \Sigma_j^{\parallel}, \Sigma_j^{\perp} \right) \right\}_{j=1}^N,
\quad \vec{o}_j, \vec{d}_j \in \mathbb{R}^D,
\quad T_j \in \mathbb{R}^C \text{ или } \mathbb{C}^C,
\quad \Sigma_j^{\parallel} > 0,
\quad \Sigma_j^{\perp} > 0
$$

Каждой проекции сопоставляется центр $\vec{o}_j$, нормализованное направление $\vec{d}_j$ (то есть $\|\vec{d}_j\| = 1$), вектор значения $T_j$, и ковариационная структура в пространстве проекций.

Обозначим объединённую координату:
$\ell_j := \begin{bmatrix} \vec{o}_j \\ \vec{d}_j \end{bmatrix} \in \mathbb{R}^{2D}$

Ковариация проекционного ядра:

$$
\Sigma_j = R_j^T \begin{bmatrix} \Sigma_j^{\parallel} & 0 \\ 0 & \Sigma_j^{\perp} I_{D-1} \end{bmatrix} R_j,
\quad R_j \in \mathrm{O}(D)
$$

Здесь $R_j$ — ортонормальная матрица, переводящая канонический базис в систему координат с осью $e_1 = \vec{d}_j$, а остальные $D-1$ векторов ортогональны $\vec{d}_j$.

---

## Математические процедуры операций

### 1. Чтение (Read)

Для запроса $\ell = (\vec{o}, \vec{d})$, где $\|\vec{d}\| = 1$, определим объединённый вектор:
$\ell := \begin{bmatrix} \vec{o} \\ \vec{d} \end{bmatrix} \in \mathbb{R}^{2D}$

Полный ответ:

$$
T(\ell) = \sum_{j=1}^N \alpha_j T_j \cdot \psi_j(\ell),
\quad \psi_j(\ell) = \mathcal{N}(\ell \mid \ell_j, \Sigma_j)
$$

Где:

$$
\mathcal{N}(x \mid \mu, \Sigma) := \frac{1}{\sqrt{(2\pi)^{2D} |\Sigma|}} \exp\left( -\tfrac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right)
$$

Параметр $\alpha_j \in [0, 1]$ обозначает актуальность/вес вклада (по умолчанию $\alpha_j = 1$, но может быть обучаемым или динамически обновляемым).

---

### 2. Запись (Create)

Добавление новой записи:

$$
\mathcal{M} \leftarrow \mathcal{M} \cup \left\{ (\vec{o}_{\text{new}}, \vec{d}_{\text{new}}, T^*, \Sigma^{\parallel}_{\text{new}}, \Sigma^{\perp}_{\text{new}}) \right\},
\quad \alpha_{\text{new}} := 1
$$

---

### 3. Обновление (Update)

Для текущего запроса $\ell$ и целевого ответа $T^*$:

Локальная корректировка активных проекций (например, тех, у кого $\psi_j(\ell) > \varepsilon$):

$$
T_j \leftarrow T_j + \eta \cdot (T^* - T(\ell)) \cdot \psi_j(\ell),
\quad \forall j \in \text{supp}(\ell)
$$

Альтернативно — нормализованное обновление:

$$
T_j \leftarrow \frac{\psi_j(\ell)}{\sum_k \psi_k(\ell)} \cdot T^*,
\quad \text{при фиксированном } \ell
$$

---

### 4. Удаление / Забывание (Delete)

Удаление проекции $j$ может быть реализовано через:

* **Обнуление**: $T_j \leftarrow 0$, $\alpha_j \leftarrow 0$
* **Затухание**: $\alpha_j \leftarrow \gamma \cdot \alpha_j$, где $\gamma \in (0,1)$
* **Деконцентрация**: $\Sigma_j^{\parallel}, \Sigma_j^{\perp} \to \infty$ — потеря локализованности

---

### 5. Поиск по смыслу (Find)

По заданному целевому смыслу $T^*$, направление взгляда определяется как взвешенный центроид по сходству значений:

$$
\ell^* = \frac{\sum_j \ell_j \cdot \exp\left( -\frac{\|T_j - T^*\|^2}{2\tau^2} \right)}{\sum_j \exp\left( -\frac{\|T_j - T^*\|^2}{2\tau^2} \right)}
$$

Если $\ell^* = (\vec{o}^*, \vec{d}^*)$, то для корректной нормализации направления необходимо:

$$
\vec{d}^* \leftarrow \frac{\vec{d}^*}{\|\vec{d}^*\|}
$$

Для повышения эффективности допускается аппроксимация по top-k ближайшим значениям по смыслу (по евклидовой норме в $T_j$).

---

## Дифференцируемость

Все операции гладки по параметрам:
$\nabla_{T_j}, \nabla_{\vec{o}_j}, \nabla_{\vec{d}_j}, \nabla_{\Sigma_j^{\parallel}}, \nabla_{\Sigma_j^{\perp}}, \nabla_{\alpha_j}, \nabla_{T^*}, \nabla_{\ell}$

Процедура поиска $\ell^*$ является гладкой функцией от всех $T_j$, $\ell_j$, и может быть дифференцирована через $T^*$ как аргумент запроса.

---

## Свойства

* Полная поддержка операций чтения, записи, обновления и удаления (CRUD)
* Аналитический и обратимый поиск по смыслу
* Волновая интерференция вкладов
* Геометрически параметризованное представление памяти
* Дифференцируемость по всем параметрам
* Возможность обучения структуры памяти end-to-end
